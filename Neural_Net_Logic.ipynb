{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "learning_rate = 1\n",
    "\n",
    "# Set bounds for the initialization of the random weights\n",
    "weight_min = 0\n",
    "weight_max = 1\n",
    "\n",
    "# Set bounds for the initialization of the random bias\n",
    "bias_min = 0\n",
    "bias_max = 1\n",
    "\n",
    "# Define the activation function\n",
    "def sigmoid(drive):\n",
    "    return 1/(1+np.exp(-drive))\n",
    "\n",
    "# Define the derivative of the activation function\n",
    "def sigmoidprime(drive):\n",
    "    return np.exp(drive)/((np.exp(drive)+1)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data - Initialize input and expected output (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we are trying to learn our network as a logic gate \n",
    "# we can denote every possible input pair as follows\n",
    "input_pairs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "# Depending on which logic gate we are considering, \n",
    "# there will be different outputs or labels/targets for each input pair.\n",
    "t_and = np.array([0,0,0,1])\n",
    "t_or = np.array([0,1,1,1])\n",
    "t_nand = np.array([1,1,1,0])\n",
    "t_nor = np.array([1,0,0,0])\n",
    "t_xor = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_units = None, activation = 0):\n",
    "        # Initialize the input units so what feeds into the perceptron\n",
    "        # this could be either an input pair if our perceptron is in the first hidden layer\n",
    "        # or it could be other perceptrons if our perceptron is in any other layer\n",
    "        self.input_units = input_units\n",
    "        \n",
    "        # Initialize random weights between weight_min and weight_max (uniformly)\n",
    "        # one weight for each input unit\n",
    "        try:\n",
    "            # We try to initialize weights for the inputs, if there are no inputs\n",
    "            # that means we are in the input layer and dont need any weights\n",
    "            #self.weights = np.random.uniform(weight_min, weight_max, len(input_units))\n",
    "            self.weights = np.random.randn(len(input_units))\n",
    "        except TypeError:\n",
    "            # no inputs means input_units = None\n",
    "            # len(None) throws a TypeError which we except here\n",
    "            # and just \"initialize\" the weights with an empty list\n",
    "            self.weights = []\n",
    "        \n",
    "        # Initalize random bias between bias_min and bias_max (uniformly)\n",
    "        # self.bias = np.random.uniform(bias_min, bias_max)\n",
    "        self.bias = np.random.randn()\n",
    "        \n",
    "        # Initialize learning rate\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        # Initialize the activation\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize the drive\n",
    "        self.drive = 0\n",
    "        \n",
    "        \n",
    "    def forward_step(self, input):\n",
    "        # Perform a perceptron forward step.\n",
    "        # 1. Calculate the drive. You can use @ as a matrix multiplication command.\n",
    "        weighted_sum =  self.weights @ input + self.bias\n",
    "        \n",
    "        # 2. Denote the drive of the perceptron\n",
    "        weighted_sum =  self.weights @ input + self.bias\n",
    "        self.drive = weighted_sum\n",
    "        \n",
    "        # 3. Denote the activation of the perceptron\n",
    "        self.activation = sigmoid(weighted_sum)\n",
    "        \n",
    "        # 4. Return the activation\n",
    "        return self.activation\n",
    "    \n",
    "    \n",
    "    def update(self, delta):\n",
    "        # delta is the error vector of the next layer\n",
    "        # delta * activation gives us the weight update\n",
    "        # the activation can be taken from the perceptrons of the previous layer\n",
    "        # if we reached the 2nd layer in the backpropagation then the previous layer\n",
    "        # will be the input layer\n",
    "        weight_update = delta * np.array([input_unit.activation for input_unit in self.input_units])\n",
    "        \n",
    "        # we update our weights according to our learning rate\n",
    "        self.weights -= self.alpha * weight_update\n",
    "        \n",
    "        # we also update the bias\n",
    "        bias_update = delta * 1\n",
    "        self.bias -= self.alpha * bias_update\n",
    "        \n",
    "    def set_input(self, input):\n",
    "        self.activation = input\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, shape = [1,1,1]):\n",
    "        self.shape = shape\n",
    "        # The shape determines the number of input, hidden and output neurons\n",
    "        # [Perceptrons() for i in range(shape[0])] allows us to initialize as many Perceptrons \n",
    "        # as we indicated in the shape\n",
    "        \n",
    "        # We model input layer units as perceptrons with no input units \n",
    "        # but only an activation, which denotes their value, so the input of the MLP\n",
    "        self.input_layer = [Perceptron() for i in range(shape[0])]\n",
    "        \n",
    "        # The input units of the hidden layer are the units of the input layer\n",
    "        self.hidden_layer = [Perceptron(input_units = self.input_layer) for h in range(shape[1])]\n",
    "        \n",
    "        # The input units of the output layer are the units of the hidden layer\n",
    "        self.output_layer = [Perceptron(input_units = self.hidden_layer) for o in range(shape[2])]\n",
    "       \n",
    "    \n",
    "    def forward_step(self, input):\n",
    "        # Input is one of our input pairs [[0,0],[0,1],[1,0],[1,1]]\n",
    "        # This input has to be propergated forward through the MLP\n",
    "        \n",
    "        # Initialize the values of the input layer (activation) with the according input\n",
    "        for input_unit, input_value in zip(self.input_layer, input):\n",
    "            input_unit.set_input(input_value)\n",
    "            \n",
    "        # Calculate the activations of the hidden layer based on the input\n",
    "        # the input is converted into a numpy array to get acces to the dot product function\n",
    "        for hidden_unit in self.hidden_layer:\n",
    "            hidden_unit.forward_step(np.array(input))\n",
    "            \n",
    "        # access the activation for each unit of the hidden layer\n",
    "        hidden_layer_activations = [hidden_unit.activation for hidden_unit in self.hidden_layer]    \n",
    "        \n",
    "        # Calculate the activations of the output layer based on the hidden layer\n",
    "        for output_unit in self.output_layer:\n",
    "            # pass the activations to the output unit\n",
    "            output_unit.forward_step(hidden_layer_activations)\n",
    "        \n",
    "        \n",
    "    def backprop_step(self, label):\n",
    "        # Calculate the error term for each unit of the ouput layer\n",
    "        # in our case that would just be one\n",
    "        delta_o = []\n",
    "        for output_unit in self.output_layer:\n",
    "            delta_o.append(-(label - output_unit.activation) * sigmoidprime(output_unit.drive))\n",
    "            \n",
    "        # Now we can update the weights and bias of our ouput units\n",
    "        for output_unit, delta in zip(self.output_layer, delta_o):\n",
    "            output_unit.update(delta)\n",
    "            \n",
    "        # Next up we calculate the error terms of the hidden layer\n",
    "        delta_h = []\n",
    "        for index, hidden_unit in enumerate(self.hidden_layer):\n",
    "            delta_h.append((delta_o[0] * self.output_layer[0].weights[index]) * sigmoidprime(hidden_unit.drive))\n",
    "            \n",
    "        # And again we update the weights and bias according to the error terms\n",
    "        for hidden_unit, delta in zip(self.hidden_layer, delta_h):\n",
    "            hidden_unit.update(delta)\n",
    "                \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a MLP on AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Mulit-Layer Perceptron with\n",
    "# 1 input layer consisting of 2 input units\n",
    "# 1 hidden layer consisting of 4 hidden units\n",
    "# 1 output layer consisting of 1 output unit\n",
    "my_MLP = MLP([2,4,1])\n",
    "\n",
    "# Choose logic gate\n",
    "target = t_xor\n",
    "\n",
    "# Initialize lists to store steps and performance.\n",
    "steps = []\n",
    "accuracies = []\n",
    "\n",
    "# Now we train the MLP for 1000 epochs and track the steps and accuracy\n",
    "for i in range(4000):\n",
    "    steps.append(i)\n",
    "    \n",
    "    # 1. Draw a random sample from the input pairs and the corresponding targets.\n",
    "    index = np.random.randint(len(input_pairs))\n",
    "    sample = input_pairs[index]\n",
    "    label = target[index]\n",
    "    \n",
    "    # 2. Perform a forward step\n",
    "    my_MLP.forward_step(sample)\n",
    "    \n",
    "    # 3. Perform a backprop-step\n",
    "    my_MLP.backprop_step(label)\n",
    "    \n",
    "    # 4. Calculate the performance over all four possible inputs.\n",
    "    accuracy_sum = 0\n",
    "    for k in range(len(input_pairs)):\n",
    "        # forward the input pair through the MLP\n",
    "        my_MLP.forward_step(input_pairs[k])\n",
    "        output = my_MLP.output_layer[0].activation\n",
    "        \n",
    "        # if the MLP's prediction is closer than 0.5 to the actual target\n",
    "        # it counts as a correct classification and\n",
    "        accuracy_sum += int(abs(output - target[k]) < 0.5)\n",
    "    \n",
    "    # we divide through all the training examples\n",
    "    accuracy = accuracy_sum / 4                  \n",
    "    accuracies.append(accuracy)   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
